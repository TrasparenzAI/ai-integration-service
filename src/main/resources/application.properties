spring.application.name=ai-integration-service

# Configurazione Spring AI per Ollama
#spring.ai.ollama.base-url=http://localhost:11434
# URL del servizio Ollama (Server con le GPU AMD, esposto sulla porta 11434)
spring.ai.ollama.base-url=http://150.145.70.201:11434
# Modello di default da usare per chat/completions
# Puoi sovrascriverlo anche con env var SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL
spring.ai.ollama.chat.options.model=mixtral:8x7b-instruct-v0.1-q5_K_M
#spring.ai.ollama.chat.options.model=llama3

# ------------------------------------------------------------
# Configurazione MCP (Model Context Protocol) come client
# ------------------------------------------------------------
# Lo starter "spring-ai-starter-mcp-client" è incluso nel pom.xml.
# Per istruire l'app a usare uno o più MCP Server (HTTP/SSE o STDIO),
# definisci blocchi "spring.ai.mcp.client.<id>.*" come negli esempi.

# Abilita MCP (puoi usare anche l'ENV SPRING_AI_MCP_ENABLED=true)
spring.ai.mcp.client=false

# Simple configuration using default /sse endpoint
spring.ai.mcp.client.streamable-http.connections.local_mcp.url: http://localhost:8081

